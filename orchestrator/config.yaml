models:
  list:
    # See all the models LiteLLM supports: https://docs.litellm.ai/docs/
    # Note: the api_key_env value is the name of the environment variable holding the actual key.
    - name: "kimi-k2"
      # OpenRouter model strings use the format: openrouter/<provider>/<model-name>
      # Full list: https://openrouter.ai/models
      model: "openrouter/moonshotai/kimi-k2.5"
      api_key_env: "OPENROUTER_API_KEY"
      max_tokens: 8192
    - name: "grok"
      model: "openrouter/x-ai/grok-code-fast-1"
      api_key_env: "OPENROUTER_API_KEY"
      max_tokens: 65536
    - name: "llama3"
      model: "ollama/llama3.2"
      api_key_env: ""
      max_tokens: 4096
    - name: "claude-sonnet"
      model: "anthropic/claude-sonnet-4-6"
      api_key_env: "ANTHROPIC_API_KEY"
      max_tokens: 8192
  # Each category maps a named model (by name in the list above) to the task
  # types it should handle. _select_model() in main.py resolves any model_key
  # ("default", "local", "remote") through this table.
  categories:
    - category: "default"
      # ORCHESTRATOR_MODEL env var overrides this at runtime without touching config.
      model: "grok"
      used_for: []
    - category: "local"
      model: "llama3"
      used_for:
        - "summarize"
        - "classify"
        - "reflect"
    - category: "remote"
      model: "kimi-k2"
      used_for:
        - "dev"
        - "research"
        - "plan"

paths:
  # Paths are evaluated relative to the working directory.
  allowed_read:
    - "./"
  allowed_write:
    - "./"
    - "~/.local/germinal/"
  db: "~/.local/germinal/orchestrator.db"
  logs: "./logs/"

tools:
  shell_allowlist:
    - "pytest"
    - "python"
    - "uv"

agents:
  task_agent:
    allowed_tools:
       - "*"
    max_iterations: 50
    approval_required_for: ["high"]

approval:
  # Terminal mode: blocks on stdin prompt. Replaced in later phases.
  mode: "terminal"

logging:
  # DEBUG emits full prompt dumps; INFO is the recommended production level.
  level: WARN

security:
  # Enable/disable security validation of tool outputs before sending to LLM
  enabled: true
  # List of validators to apply (in order). Available: sensitive_data_masker, prompt_injection_detector
  validators:
    - "sensitive_data_masker"
    - "prompt_injection_detector"

context:
  # when the combined context history exceeds these many tokens (currently 1 token == 4 characters) then invoke
  # a llm call to summarise the history, save it in the projects table in the db, and wipe the context history.
  recent_buffer_tokens: 4000
  summary_tokens: 1000
  brief_tokens: 500

input:
  # Limits for piped input in oneshot mode (e.g., cat file | germ "analyze this")
  max_file_size_mb: 100
  max_tokens_estimate: 200000  # Conservative limit below typical 250k context windows
  large_file_threshold_mb: 10   # Files above this size trigger warnings
  enable_content_tools: true    # Allow tools to access large content incrementally

projects:
  # All events without an explicit project_id accumulate history here.
  # Set to null to disable automatic project assignment.
  default_project_id: "default"
  default_project_name: "Default Project"

network:
  # Set enabled: false to run without any network listener (timer-only mode).
  enabled: true
  tcp:
    host: "127.0.0.1"
    port: 8080
  # UNIX domain socket path. Clients (e.g. curl --unix-socket) connect here.
  # Set to null to disable the UNIX socket listener.
  unix_socket: "/tmp/germinal.sock"
  # Seconds to wait for an agent response before returning a 504 to the HTTP client.
  request_timeout_s: 300
  # Set require_auth: true and provide an api_key to require Bearer token authentication.
  # Clients must send: Authorization: Bearer <api_key>
  require_auth: false
  api_key: ""
  # Name shown to clients in GET /v1/models and echoed back in responses.
  # The client must send this string in the model field (required by the OpenAI
  # protocol) but the orchestrator ignores it for routing and LLM selection.
  model_name: "germinal"
